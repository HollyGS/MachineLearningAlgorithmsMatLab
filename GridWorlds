function [S, A, T, R, StateNames, ActionNames, AbsorbingStates] = RunCoursework()
close all
gamma = 0.45; %Discount Factor
p = 0.85;     %Probability of moving in desired direction
q = (1-p)/2; %Probability of moving adjacent to desired direction
trials = 40; % Set number of trials for E-Greedy Method
[NumStates, NumActions, TransitionMatrix, ...
    RewardMatrix, StateNames, ActionNames, AbsorbingStates] ...
    = PersonalisedGridWorld(p);
% Unbiased policy
Policy = [
    %  N    E    S    W
    0.25 0.25 0.25 0.25; % 1
    0.25 0.25 0.25 0.25; % 2
    0.25 0.25 0.25 0.25; % 3
    0.25 0.25 0.25 0.25; % 4
    0.25 0.25 0.25 0.25; % 5
    0.25 0.25 0.25 0.25; % 6
    0.25 0.25 0.25 0.25; % 7
    0.25 0.25 0.25 0.25; % 8
    0.25 0.25 0.25 0.25; % 9
    0.25 0.25 0.25 0.25; % 10
    0.25 0.25 0.25 0.25; % 11
    0.25 0.25 0.25 0.25; % 12
    0.25 0.25 0.25 0.25; % 13
    0.25 0.25 0.25 0.25]; % 14

Action = [
    % 1  2  3  4  5  6  7  8  9  10  11  12  13  14 = priorStates
    5  0  0  0  1  0  0  0  0  0   0   0   0   0; % 5 = N || W
    2  0  0  0  0  1  0  0  0  0   0   0   0   0;
    0  0  0  4  0  0  1  0  0  0   0   0   0   0;
    0  0  0  6  0  0  0  1  0  0   0   0   0   0; % 6 = N || E
    3  0  0  0  4  4  0  0  1  0   0   0   0   0;
    0  0  0  0  2  3  4  0  0  0   0   0   0   0;
    0  0  0  0  0  2  3  4  0  0   0   0   0   0;
    0  0  0  3  0  0  2  2  0  1   0   0   0   0;
    0  0  0  0  3  0  0  0  7  0   1   0   0   0; % 7 = E || W
    0  0  0  0  0  0  0  0  0  7   0   0   0   1; % 7 = E || W
    0  0  0  0  0  0  0  3  3  0   8   4   0   0; % 8 = S || W
    0  0  0  0  0  0  0  0  0  0   2   9   4   0; % 9 = N || S
    0  0  0  0  0  0  0  0  0  0   0   2   9   4;
    0  0  0  0  0  0  0  0  0  3   0   0   2   10];%10 = E || S

[V] = PolicyEvaluation(Policy, TransitionMatrix, RewardMatrix, AbsorbingStates, gamma);

[Tr] = MonteCarlo(TransitionMatrix, AbsorbingStates, gamma);

EGreedy(TransitionMatrix, AbsorbingStates, gamma, trials);
%-----------------------------------------------------------------------------------
    function [V] = PolicyEvaluation(Policy, TransitionMatrix, RewardMatrix, AbsorbingStates, gamma)
      
        S = length(Policy); %Number of States
        A = length(Policy(1,:)); %Number of Actions
        V = zeros(S, 1); %Initialise Value Function for MDP
        newV = V;
        tol = 0.0001;
        delta = tol * 2;
        Q2 = [];

        while delta > tol
            for priorState = 1 : S
                if AbsorbingStates(priorState) == 1 % Do not update absorbing states
                    continue;
                end
                tmpV = 0;
                for action = 1 : A
                    tmpQ = 0;
                    for postState = 1 : S
                        tmpQ = tmpQ + TransitionMatrix(postState,priorState,action)*(RewardMatrix(postState,priorState,action) + gamma * V(postState)); %Bellmann Optimality Equation for Q
                    end
                    tmpV = tmpV + Policy(priorState,action) * tmpQ;
                end
                newV(priorState) = tmpV;
            end
            diffVec = abs(V - newV);
            delta = max(diffVec);
            V = newV;
            Q2 = [Q2;V];
        end
disp('Q2 Value Function:')
disp(V')
    end



    function [Tr] = MonteCarlo(TransitionMatrix, AbsorbingStates, gamma)
        
        V10 = zeros(14,1); % Initialise value function for 10 traces
        mTr = []; % Averaged value functions with increasing trace number

        for Traces = 1:10
            
            % Generate Trace
            
            currState = randi([11 14], 1,1); % Start at State 11, 12, 13 or 14
            Tr = [currState;]; % Store first state in trace
            State = zeros(14,1); % Position Vector
            State(currState) = 1; % Set initial State in position vector
            Rew = [-1;];
            TrV = [];
            A = [];
            
            while AbsorbingStates(currState) == 0
                x = randi([1 4],1,1); % Determine desired direction (unbiased policy)
                T = TransitionMatrix(:,:,x);
                State = T*State; % Use transition matrix to determine successor state
                tmpS = State;
                
                for i = 2:14
                    tmpS(i) = tmpS(i)+tmpS(i-1); % Cumulative probability for successor state
                end        
                y = rand(1,1);
                State = zeros(14,1);
                % Set successor state in position vector
                for z = 1:14
                    if any(State) == 0
                        if y < tmpS(z)
                            State(z) = 1;
                            postState = z;
                        end
                    end
                end
                
                [~,currState] = max(State'); % Position of current state
                Tr = [Tr;currState]; % Record current state in trace vector
               
                
               % Generate Reward Vector 
               
                if currState == 2
                    r = 0;
                elseif currState == 3
                    r = -10;
                else
                    r = -1;
                end
                Rew = [Rew;r]; % Store rewards for each state in trace
            end            
            disp('Q4(a) Trace:')
            disp(Tr')
            % Policy evaluation
            
            TrV = []; % Empty Value Function vector for current trace
            for j = 2:length(Rew) % For every state in trace
                Ret = 0; % Initialise state value
                for i = j:length(Rew)
                    Ret = Rew(i)*gamma^(i-j) + Ret; % Calculate state value
                end
                TrV = [TrV;Ret]; % Store state return in indivual trace value function
            end    
            
            TrV = [TrV;0]; % Value of terminal state is zero
            VF = zeros(14,1); % Initialise value function for new trace           
            for L = 1:length(Tr)
                if VF(Tr(L)) == 0 % Only store value of state on first visit
                    VF(Tr(L)) = TrV(L);
                    if V10(Tr(L)) == 0
                        V10(Tr(L)) = VF(Tr(L)); % Update V10 for first trace
                    else
                        V10(Tr(L)) = (V10(Tr(L)) + VF(Tr(L)))/2;   % Update V10 with average of 2 or more traces value funcitions
                    end
                end
            end
            mTr = [mTr;mean(V10)]; % Store value function for every number of traces

            
            
            % Generate Action sequence
            if Action(postState,currState) == 0 % Absorbing state
                            continue;
                        elseif Action(postState,currState) < 5 % Move to new state
                            a = Action(postState,currState);
                        else                                   % Bounce of wall
                            if Action(postState,currState) == 7
                                if x == 2 || 4
                                    a = x;
                                else
                                    n = rand(1,1);
                                    if n < 0.5
                                        a = 1;
                                    else
                                        a = 3;
                                    end
                                end
                            elseif Action(postState,currState) == 9
                                if x == 1 || 3
                                    a = x;
                                else
                                    n = rand(1,1);
                                    if n < 0.5
                                        a = 2;
                                    else
                                        a = 4;
                                    end
                                end
                            else
                                n = randi([1 37],1,1); % Probability that move is in desired direction given option of 2 directions is p/(p+q) = 34/37
                                if Action(postState,currState) == 5
                                    if x == 2 || (x == 1 && n < 35) || (x == 4 && n > 34) % If desired direction is 2 (East) then action cannot be 4 (West)
                                        a = 1;
                                    else % x == 3 || (x == 4 && n < 35) || (x == 1 && n > 34)
                                        a = 4;
                                    end
                                elseif Action(postState,currState) == 6
                                    if x == 3 || (x == 1 && n < 35) || (x == 2 && n > 34)
                                        a = 1;
                                    else
                                        a = 2;
                                    end
                                elseif Action(postState,currState) == 8
                                    if x == 2 || (x == 3 && n < 35) || (x == 4 && n > 34)
                                        a = 3;
                                    else
                                        a = 4;
                                    end
                                else
                                    if x == 1 || (x == 2 && n < 35) || (x == 3 && n > 34)
                                        a = 2;
                                    else
                                        a = 3;
                                    end
                                end                               
                            end
                        end
                        
                        A = [A;a];

        end
        disp('Value Function:')
        disp(V10')
 
                % Value Function Similarity Plot Q4c
        figure(1);
        m = mean(V) .* [1; 1; 1; 1; 1; 1; 1; 1; 1; 1];
        valdiff = mTr - m;
        plot(valdiff,'LineWidth',2)
        xlabel('Traces')
        ylabel('Difference between mean of Value Functions')
        title('Q 4(c): Similarity of Dynamic Programming and Monte-Carlo Value Functions') 
        
        
        end
        

    function [EV] = EGreedy(TransitionMatrix, AbsorbingStates, gamma, trials)
        Q = zeros(14,4); % Average(returns(s,a))
        Pi = Policy; % Initialise Pi
        TriRew1 = []; % Empty Rewards for each trial
        TriRew2 = [];
        TriL1 = []; % Average Trace lengths for each trial
        TriL2 = [];
        
        for t = 1:trials
            TrRew1 = []; % Rewards for each trace
            TrRew2 = [];
            TrL1 = []; % Length of each trace
            TrL2 = [];
            
            for E = [0.1 0.75] % Two settings of epsilon
                explore = E/4; % Probability of exploring action
                exploit = 1 - E + E/4; % Probability of exploiting known best action
                for Traces = 1:10
                    currState = randi([11 14], 1,1); % Start at State 11, 12, 13 or 14
                    Tr = [currState;]; % Store first trace
                    State = zeros(14,1);
                    State(currState) = 1; % Set initial State in vector
                    Rew = [-1;]; % Store reward of first state
                    A = []; % Action sequence vector
                    Ret = zeros(14,4); % Returns for each (s,a)
                     
                    %Generate Trace
                    while AbsorbingStates(currState) == 0
                        
                        tmpPi = Pi; %Set policy
                        for i = 1:14
                            for j = 2:4
                                tmpPi(i,j) = tmpPi(i,j) + tmpPi(i,j-1); %cumalative probability policy matrix
                            end
                        end
                        
                        w = rand(1,1); 
                        x = 0;
                        s = currState;
                        for j = 1:4
                            if x == 0
                                if w < tmpPi(s,j)
                                    x = j; % Intended Direction N = 1, E = 2, S = 3, W = 4
                                end
                            else
                                continue;
                            end
                        end
                        
                        T = TransitionMatrix(:,:,x);
                        State = T*State;
                        tmpS = State;
                        
                        for i = 2:14
                            tmpS(i) = tmpS(i)+tmpS(i-1); %cumulative probability transition matrix
                        end
                        
                        y = rand(1,1);
                        State = zeros(14,1);
                        % generate successor state
                        for z = 1:14
                            if any(State) == 0
                                if y < tmpS(z)
                                    State(z) = 1;
                                    postState = z;
                                end
                            end
                        end
                        
                        if Action(postState,currState) == 0 % Absorbing state
                            continue;
                        elseif Action(postState,currState) < 5 % Move to new state
                            a = Action(postState,currState);
                        else                                   % Bounce of wall
                            if Action(postState,currState) == 7
                                if x == 2 || 4
                                    a = x;
                                else
                                    n = rand(1,1);
                                    if n < 0.5
                                        a = 1;
                                    else
                                        a = 3;
                                    end
                                end
                            elseif Action(postState,currState) == 9
                                if x == 1 || 3
                                    a = x;
                                else
                                    n = rand(1,1);
                                    if n < 0.5
                                        a = 2;
                                    else
                                        a = 4;
                                    end
                                end
                            else
                                n = randi([1 37],1,1); % Probability that move is in desired direction given option of 2 directions is p/(p+q) = 34/37
                                if Action(postState,currState) == 5
                                    if x == 2 || (x == 1 && n < 35) || (x == 4 && n > 34) % If desired direction is 2 (East) then action cannot be 4 (West)
                                        a = 1;
                                    else % x == 3 || (x == 4 && n < 35) || (x == 1 && n > 34)
                                        a = 4;
                                    end
                                elseif Action(postState,currState) == 6
                                    if x == 3 || (x == 1 && n < 35) || (x == 2 && n > 34)
                                        a = 1;
                                    else
                                        a = 2;
                                    end
                                elseif Action(postState,currState) == 8
                                    if x == 2 || (x == 3 && n < 35) || (x == 4 && n > 34)
                                        a = 3;
                                    else
                                        a = 4;
                                    end
                                else
                                    if x == 1 || (x == 2 && n < 35) || (x == 3 && n > 34)
                                        a = 2;
                                    else
                                        a = 3;
                                    end
                                end                               
                            end
                        end
                        
                        A = [A;a];
                        [~,currState] = max(State'); % position of current state
                        Tr = [Tr;currState]; % Record current state in trace vector %episode!                       
                        if currState == 2
                            r = 0;
                        elseif currState == 3
                            r = -10;
                        else
                            r = -1;
                        end
                        Rew = [Rew;r]; % Vector of rewards for each state in trace
                    end                   
                    
                    G = []; %Empty Value Function Matrix
                    for j = 2:length(Rew) % For each state
                        TrV = 0;
                        for i = j:length(Rew)
                            TrV = Rew(i)*gamma^(i-j) + TrV; %Calculate Trace Value Function
                        end
                        G = [G;TrV];
                    end
                    
                    G = [G;0];
                    A = [A;1];
                    for L = 1:length(Tr)
                        if Ret(Tr(L),A(L)) == 0 % First Batch
                            Ret(Tr(L),A(L)) = G(L); %Append G to returns, which is for each trace
                            if Q(Tr(L),A(L)) == 0
                                Q(Tr(L),A(L)) = Ret(Tr(L),A(L)); %update Q first time: Q = average(returns(s,a))
                            else
                                Q(Tr(L),A(L)) = (Q(Tr(L),A(L)) + Ret(Tr(L),A(L)))/2;   %update Q with average
                            end
                        end
                    end
                                        
                    for  s = 1:length(Q)
                        [M,I] = max(Q(s,:));
                        Pi(s,I) = exploit;
                        for i = setdiff(1:4, I)
                            Pi(s,i) = explore;
                        end
                    end
                    
                    if E == 0.1
                        TrRew1 = [TrRew1;sum(Rew)];
                        TrL1 = [TrL1;length(Tr)];
                    else
                        TrRew2 = [TrRew2;sum(Rew)];
                        TrL2 = [TrL2;length(Tr)];
                    end                                     
                end
            end
            
            TriRew1 = cat(2,TriRew1,TrRew1);
            TriRew2 = cat(2,TriRew2,TrRew2);
            TriL1 = cat(2,TriL1,TrL1);
            TriL2 = cat(2,TriL2,TrL2);
        end
       
        
        a = zeros(10,1);
        b = zeros(10,1);
        c = zeros(10,1);
        d = zeros(10,1);

        for t = 1:10
            a(t) = mean(TriRew1(t,:));
            err1(t) = std(TriRew1(t,:));
            b(t) = mean(TriRew2(t,:));
            err2(t) = std(TriRew2(t,:));
            c(t) = mean(TriL1(t,:));
            err3(t) = std(TriL1(t,:));
            d(t) = mean(TriL2(t,:));
            err4(t) = std(TriL2(t,:));
            
        end
        
        
        figure(2);
        
        subplot(2,2,1)
        errorbar(a,err1,'LineWidth',1.2)
        xlabel('Episodes','fontsize',15)
        ylabel('Reward','fontsize',15)
        title('Q5 Rewards for E = 0.1','fontsize',20)
        
        subplot(2,2,2)
        errorbar(b,err2,'LineWidth',1.2)
        xlabel('Episodes','fontsize',15)
        ylabel('Reward','fontsize',15)
        title('Rewards for E = 0.75','fontsize',20)
     
        subplot(2,2,3)
        errorbar(c,err3,'LineWidth',1.2)
        xlabel('Episodes','fontsize',15)
        ylabel('Trace Length','fontsize',15)
        title('Trace length for E = 0.1','fontsize',20)

        subplot(2,2,4)
        errorbar(d,err4,'LineWidth',1.2)
        xlabel('Episodes','fontsize',15)
        ylabel('Trace Length','fontsize',15)
        title('Trace length for E = 0.75','fontsize',20)
    end
end
